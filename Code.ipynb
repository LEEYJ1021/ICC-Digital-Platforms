{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4452e167-f5ed-4541-adc3-21b5406ef660",
   "metadata": {},
   "outputs": [],
   "source": [
    "###'STAGE 1: AMENITY ANALYSIS' & 'STAGE 2: LISTING CLUSTERING'###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8086029-579c-4303-9c02-194bb7855042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Detected encoding for C:\\Users\\USER\\Downloads\\ICC in Digital Platforms\\reviews.csv: latin1\n",
      "Detected encoding for C:\\Users\\USER\\Downloads\\ICC in Digital Platforms\\listings.csv: latin1\n",
      "Loaded 2928 listings\n",
      "Loaded 71411 reviews\n",
      "Loaded 18 neighbourhoods\n",
      "Preprocessing data...\n",
      "After cleaning: 2927 listings\n",
      "\n",
      "=== Stage 1: Amenity Analysis ===\n",
      "Parsing amenities...\n",
      "Creating amenity co-occurrence network...\n",
      "Filtered to 168 amenities for network analysis\n",
      "[Saved table] C:\\Users\\USER\\Downloads\\ICC in Digital Platforms\\STAGE 1&2_outputs\\amenity_frequencies.csv\n",
      "Detecting amenity communities...\n",
      "Detected 2 communities\n",
      "Community 0: 55 amenities\n",
      "Community 1: 113 amenities\n",
      "[Saved table] C:\\Users\\USER\\Downloads\\ICC in Digital Platforms\\STAGE 1&2_outputs\\amenity_communities.csv\n",
      "[Saved figure] C:\\Users\\USER\\Downloads\\ICC in Digital Platforms\\STAGE 1&2_outputs\\fig_amenity_network.png\n",
      "[Saved table] C:\\Users\\USER\\Downloads\\ICC in Digital Platforms\\STAGE 1&2_outputs\\stage1_amenity_features.csv\n",
      "\n",
      "=== Stage 2: Listing Clustering ===\n",
      "Finding optimal clustering solution...\n",
      "Best solution: kmeans with k=2\n",
      "Composite score: 0.900\n",
      "[Saved table] C:\\Users\\USER\\Downloads\\ICC in Digital Platforms\\STAGE 1&2_outputs\\clustering_model_comparison.csv\n",
      "Performing bootstrap stability analysis...\n",
      "Bootstrap stability - Mean ARI: 0.960 ± 0.023\n",
      "[Saved table] C:\\Users\\USER\\Downloads\\ICC in Digital Platforms\\STAGE 1&2_outputs\\clustering_bootstrap_ari.csv\n",
      "Profiling clusters...\n",
      "\n",
      "Cluster 0:\n",
      "  Size: 1204 (41.1%)\n",
      "  Average amenities: 23.6\n",
      "\n",
      "Cluster 1:\n",
      "  Size: 1723 (58.9%)\n",
      "  Average amenities: 10.8\n",
      "[Saved table] C:\\Users\\USER\\Downloads\\ICC in Digital Platforms\\STAGE 1&2_outputs\\cluster_profiles.csv\n",
      "[Saved figure] C:\\Users\\USER\\Downloads\\ICC in Digital Platforms\\STAGE 1&2_outputs\\fig_clustering_results.png\n",
      "\n",
      "=== COMPREHENSIVE ICC ANALYSIS REPORT ===\n",
      "==================================================\n",
      "=== COMPREHENSIVE ICC ANALYSIS REPORT ===\n",
      "==================================================\n",
      "Dataset Overview:\n",
      "- Total listings analyzed: 2927\n",
      "- Price range: $85 - $150000\n",
      "- Average price: $784\n",
      "\n",
      "Listing Clusters:\n",
      "- Cluster 0: 1204 listings (41.1%)\n",
      "- Cluster 1: 1723 listings (58.9%)\n",
      "\n",
      "Amenity Categories Summary:\n",
      "- Accessibility: 0.52 average amenities\n",
      "- Bathroom_Personal: 3.08 average amenities\n",
      "- Bedding_Comfort: 2.32 average amenities\n",
      "- Business_Work: 1.53 average amenities\n",
      "- Climate_Control: 1.17 average amenities\n",
      "- Essential_Tech: 0.80 average amenities\n",
      "- Kitchen_Dining: 3.62 average amenities\n",
      "- Outdoor_Recreation: 0.50 average amenities\n",
      "- Safety_Security: 2.04 average amenities\n",
      "- Transportation: 0.23 average amenities\n",
      "- Family_Pets: 0.27 average amenities\n",
      "\n",
      "Analysis completed successfully!\n",
      "==================================================\n",
      "[Saved report] C:\\Users\\USER\\Downloads\\ICC in Digital Platforms\\STAGE 1&2_outputs\\icc_comprehensive_report.txt\n",
      "\n",
      "Stage 1 & 2 Analysis completed successfully!\n",
      "\n",
      "Analysis completed. Results stored in 'results' variable.\n",
      "Access the processed data via: results.df_listings\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import chardet\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =========================\n",
    "# OUTPUT CONFIG\n",
    "# =========================\n",
    "output_dir = r\"C:\\Users\\USER\\Downloads\\ICC in Digital Platforms\\STAGE 1&2_outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def save_figure(fig, filename, dpi=300):\n",
    "    \"\"\"Save matplotlib figure to output_dir.\"\"\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    fig.savefig(filepath, dpi=dpi, bbox_inches='tight')\n",
    "    print(f\"[Saved figure] {filepath}\")\n",
    "\n",
    "def save_dataframe(df, filename, index=True):\n",
    "    \"\"\"Save DataFrame as CSV to output_dir.\"\"\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    df.to_csv(filepath, index=index, encoding='utf-8-sig')\n",
    "    print(f\"[Saved table] {filepath}\")\n",
    "\n",
    "def append_log(message, filename=\"icc_log.txt\"):\n",
    "    \"\"\"Append a line of text to a log file in output_dir.\"\"\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    with open(filepath, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(str(message) + \"\\n\")\n",
    "\n",
    "\n",
    "# File paths (using your existing paths)\n",
    "files = {\n",
    "    \"reviews\": r\"C:\\Users\\USER\\Downloads\\ICC in Digital Platforms\\reviews.csv\",\n",
    "    \"neighbourhoods_csv\": r\"C:\\Users\\USER\\Downloads\\ICC in Digital Platforms\\neighbourhoods.csv\",\n",
    "    \"listings\": r\"C:\\Users\\USER\\Downloads\\ICC in Digital Platforms\\listings.csv\",\n",
    "    \"neighbourhoods_geojson\": r\"C:\\Users\\USER\\Downloads\\ICC in Digital Platforms\\neighbourhoods.geojson\",\n",
    "    \"calendar\": r\"C:\\Users\\USER\\Downloads\\ICC in Digital Platforms\\calendar.csv.gz\"\n",
    "}\n",
    "\n",
    "# Your existing helper functions\n",
    "def detect_encoding(file_path, n_bytes=10000):\n",
    "    \"\"\"Detect the file encoding by reading the first n_bytes.\"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read(n_bytes)\n",
    "    result = chardet.detect(raw_data)\n",
    "    encoding = result['encoding']\n",
    "    if encoding is None or encoding.lower() == 'ascii':\n",
    "        encoding = 'latin1'\n",
    "    return encoding\n",
    "\n",
    "def safe_read_csv(file_path, is_gzip=False, **kwargs):\n",
    "    \"\"\"Safely read CSV files with encoding detection.\"\"\"\n",
    "    encoding = detect_encoding(file_path)\n",
    "    print(f\"Detected encoding for {file_path}: {encoding}\")\n",
    "    \n",
    "    try:\n",
    "        if is_gzip:\n",
    "            with gzip.open(file_path, 'rt', encoding=encoding, errors='replace') as f:\n",
    "                return pd.read_csv(f, on_bad_lines='skip', **kwargs)\n",
    "        else:\n",
    "            with open(file_path, 'r', encoding=encoding, errors='replace') as f:\n",
    "                return pd.read_csv(f, on_bad_lines='skip', **kwargs)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path} with encoding {encoding}: {e}\")\n",
    "        if is_gzip:\n",
    "            with gzip.open(file_path, 'rt', encoding='latin1', errors='replace') as f:\n",
    "                return pd.read_csv(f, on_bad_lines='skip', **kwargs)\n",
    "        else:\n",
    "            with open(file_path, 'r', encoding='latin1', errors='replace') as f:\n",
    "                return pd.read_csv(f, on_bad_lines='skip', **kwargs)\n",
    "\n",
    "\n",
    "class ICCAnalysisPipeline:\n",
    "    \"\"\"Complete pipeline for Information-Context Complementarity analysis\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.df_listings = None\n",
    "        self.df_reviews = None\n",
    "        self.gdf_neighbourhoods = None\n",
    "        self.amenity_categories = self._define_amenity_categories()\n",
    "        \n",
    "    def _define_amenity_categories(self):\n",
    "        \"\"\"Define the 11-category amenity taxonomy based on research\"\"\"\n",
    "        return {\n",
    "            'Accessibility': ['step-free access', 'accessible', 'wheelchair', 'elevator', 'wide doorway'],\n",
    "            'Bathroom_Personal': ['shampoo', 'conditioner', 'body soap', 'hot water', 'hair dryer', \n",
    "                                'towels', 'toilet paper'],\n",
    "            'Bedding_Comfort': ['bed linens', 'extra pillows', 'blankets', 'essentials', 'hangers'],\n",
    "            'Business_Work': ['wifi', 'laptop workspace', 'ethernet', 'dedicated workspace'],\n",
    "            'Climate_Control': ['air conditioning', 'heating', 'ceiling fan', 'portable fans'],\n",
    "            'Essential_Tech': ['tv', 'cable tv', 'sound system', 'speaker'],\n",
    "            'Kitchen_Dining': ['kitchen', 'refrigerator', 'microwave', 'oven', 'stove', 'dishwasher',\n",
    "                             'dishes', 'cooking basics', 'dining table', 'coffee maker', 'wine glasses'],\n",
    "            'Outdoor_Recreation': ['balcony', 'patio', 'garden', 'pool', 'hot tub', 'bbq grill',\n",
    "                                 'outdoor furniture', 'beach essentials'],\n",
    "            'Safety_Security': ['smoke alarm', 'carbon monoxide alarm', 'fire extinguisher',\n",
    "                              'first aid kit', 'security cameras', 'lock'],\n",
    "            'Transportation': ['free parking', 'paid parking', 'ev charger', 'bike'],\n",
    "            'Family_Pets': ['crib', 'high chair', 'pack n play', 'baby bath', 'pets allowed',\n",
    "                          'cat litter box', 'dog bowls']\n",
    "        }\n",
    "    \n",
    "    def load_data(self, files_dict):\n",
    "        \"\"\"Load all datasets\"\"\"\n",
    "        print(\"Loading datasets...\")\n",
    "        \n",
    "        # Load main datasets\n",
    "        self.df_reviews = safe_read_csv(files_dict[\"reviews\"])\n",
    "        self.df_listings = safe_read_csv(files_dict[\"listings\"])\n",
    "        self.gdf_neighbourhoods = gpd.read_file(files_dict[\"neighbourhoods_geojson\"])\n",
    "        \n",
    "        print(f\"Loaded {len(self.df_listings)} listings\")\n",
    "        print(f\"Loaded {len(self.df_reviews)} reviews\")\n",
    "        print(f\"Loaded {len(self.gdf_neighbourhoods)} neighbourhoods\")\n",
    "        \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"Clean and preprocess the data\"\"\"\n",
    "        print(\"Preprocessing data...\")\n",
    "        \n",
    "        # Convert price to numeric\n",
    "        if 'price' in self.df_listings.columns:\n",
    "            self.df_listings['price'] = pd.to_numeric(self.df_listings['price'], errors='coerce')\n",
    "            self.df_listings['log_price'] = np.log(self.df_listings['price'])\n",
    "        \n",
    "        # Create log transformations for key variables\n",
    "        if 'accommodates' in self.df_listings.columns:\n",
    "            self.df_listings['log_accommodates'] = np.log(self.df_listings['accommodates'] + 1)\n",
    "        if 'bedrooms' in self.df_listings.columns:\n",
    "            self.df_listings['log_bedrooms'] = np.log(self.df_listings['bedrooms'] + 1)\n",
    "        \n",
    "        # Clean amenities field\n",
    "        if 'amenities' in self.df_listings.columns:\n",
    "            self.df_listings['amenities'] = self.df_listings['amenities'].fillna('')\n",
    "        \n",
    "        # Remove listings with missing critical information\n",
    "        critical_cols = ['price', 'longitude', 'latitude']\n",
    "        available_cols = [col for col in critical_cols if col in self.df_listings.columns]\n",
    "        self.df_listings = self.df_listings.dropna(subset=available_cols)\n",
    "        \n",
    "        print(f\"After cleaning: {len(self.df_listings)} listings\")\n",
    "        \n",
    "    def stage1_amenity_analysis(self):\n",
    "        \"\"\"Stage 1: SNA-Based Feature Engineering and Amenity Community Detection\"\"\"\n",
    "        print(\"\\n=== Stage 1: Amenity Analysis ===\")\n",
    "        \n",
    "        # Parse amenities\n",
    "        self.df_listings = self._parse_amenities(self.df_listings)\n",
    "        \n",
    "        # Create amenity co-occurrence network\n",
    "        amenity_network = self._create_amenity_network()\n",
    "        \n",
    "        # Detect communities\n",
    "        communities = self._detect_amenity_communities(amenity_network)\n",
    "        \n",
    "        # Visualize network\n",
    "        self._visualize_amenity_network(amenity_network, communities)\n",
    "        \n",
    "        # Save derived data\n",
    "        amenity_cols = [c for c in self.df_listings.columns if c.startswith(\"Profile_Category_\")]\n",
    "        save_dataframe(self.df_listings[amenity_cols + ['price', 'log_price', 'longitude', 'latitude']],\n",
    "                       \"stage1_amenity_features.csv\")\n",
    "        \n",
    "        return amenity_network, communities\n",
    "    \n",
    "    def _parse_amenities(self, df):\n",
    "        \"\"\"Parse and categorize amenities\"\"\"\n",
    "        print(\"Parsing amenities...\")\n",
    "        \n",
    "        # Initialize category count columns\n",
    "        for category in self.amenity_categories:\n",
    "            df[f'Profile_Category_{category}_Count'] = 0\n",
    "        \n",
    "        # Parse amenities string and count by category\n",
    "        for idx, row in df.iterrows():\n",
    "            if pd.isna(row['amenities']):\n",
    "                continue\n",
    "                \n",
    "            amenities_list = str(row['amenities']).lower().split(',')\n",
    "            amenities_list = [a.strip().strip('\"{}[]') for a in amenities_list]\n",
    "            \n",
    "            for category, keywords in self.amenity_categories.items():\n",
    "                count = 0\n",
    "                for keyword in keywords:\n",
    "                    count += sum(1 for amenity in amenities_list if keyword in amenity)\n",
    "                df.at[idx, f'Profile_Category_{category}_Count'] = count\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _create_amenity_network(self):\n",
    "        \"\"\"Create amenity co-occurrence network\"\"\"\n",
    "        print(\"Creating amenity co-occurrence network...\")\n",
    "        \n",
    "        # Extract unique amenities\n",
    "        all_amenities = set()\n",
    "        for amenities_str in self.df_listings['amenities'].dropna():\n",
    "            amenities_list = str(amenities_str).lower().split(',')\n",
    "            amenities_list = [a.strip().strip('\"{}[]') for a in amenities_list]\n",
    "            all_amenities.update(amenities_list)\n",
    "        \n",
    "        # Filter amenities (remove very rare and very common)\n",
    "        amenity_counts = {}\n",
    "        for amenity in all_amenities:\n",
    "            count = self.df_listings['amenities'].str.contains(amenity, case=False, na=False).sum()\n",
    "            amenity_counts[amenity] = count\n",
    "        \n",
    "        total_listings = len(self.df_listings)\n",
    "        filtered_amenities = [\n",
    "            amenity for amenity, count in amenity_counts.items()\n",
    "            if 0.01 * total_listings <= count <= 0.95 * total_listings\n",
    "        ]\n",
    "        \n",
    "        print(f\"Filtered to {len(filtered_amenities)} amenities for network analysis\")\n",
    "        \n",
    "        # Save amenity frequency table\n",
    "        amenity_freq_df = (pd.Series(amenity_counts, name=\"count\")\n",
    "                           .to_frame()\n",
    "                           .assign(freq=lambda d: d['count'] / total_listings)\n",
    "                           .sort_values(\"count\", ascending=False))\n",
    "        save_dataframe(amenity_freq_df, \"amenity_frequencies.csv\")\n",
    "        \n",
    "        # Create co-occurrence matrix\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(filtered_amenities)\n",
    "        \n",
    "        # Add edges based on co-occurrence\n",
    "        for _, row in self.df_listings.iterrows():\n",
    "            if pd.isna(row['amenities']):\n",
    "                continue\n",
    "            amenities_list = str(row['amenities']).lower().split(',')\n",
    "            amenities_list = [a.strip().strip('\"{}[]') for a in amenities_list]\n",
    "            present_amenities = [a for a in amenities_list if a in filtered_amenities]\n",
    "            \n",
    "            # Add edges between all pairs of amenities in this listing\n",
    "            for i in range(len(present_amenities)):\n",
    "                for j in range(i + 1, len(present_amenities)):\n",
    "                    if G.has_edge(present_amenities[i], present_amenities[j]):\n",
    "                        G[present_amenities[i]][present_amenities[j]]['weight'] += 1\n",
    "                    else:\n",
    "                        G.add_edge(present_amenities[i], present_amenities[j], weight=1)\n",
    "        \n",
    "        # Save basic network stats\n",
    "        net_stats = {\n",
    "            \"n_nodes\": G.number_of_nodes(),\n",
    "            \"n_edges\": G.number_of_edges(),\n",
    "            \"density\": nx.density(G)\n",
    "        }\n",
    "        append_log(f\"Amenity network stats: {net_stats}\")\n",
    "        \n",
    "        return G\n",
    "    \n",
    "    def _detect_amenity_communities(self, G):\n",
    "        \"\"\"Detect communities using Louvain algorithm\"\"\"\n",
    "        print(\"Detecting amenity communities...\")\n",
    "        \n",
    "        try:\n",
    "            import networkx.algorithms.community as nx_comm\n",
    "            communities = nx_comm.louvain_communities(G, seed=42)\n",
    "        except Exception as e:\n",
    "            append_log(f\"Louvain failed, fallback to single community: {e}\")\n",
    "            communities = [set(G.nodes())]\n",
    "        \n",
    "        print(f\"Detected {len(communities)} communities\")\n",
    "        for i, community in enumerate(communities):\n",
    "            print(f\"Community {i}: {len(community)} amenities\")\n",
    "            if len(community) <= 10:  # Show small communities\n",
    "                print(f\"  Sample amenities: {list(community)[:5]}\")\n",
    "        \n",
    "        # Save community assignments\n",
    "        comm_records = []\n",
    "        for i, comm in enumerate(communities):\n",
    "            for amen in comm:\n",
    "                comm_records.append({\"amenity\": amen, \"community_id\": i})\n",
    "        comm_df = pd.DataFrame(comm_records)\n",
    "        save_dataframe(comm_df, \"amenity_communities.csv\")\n",
    "        \n",
    "        return communities\n",
    "    \n",
    "    def _visualize_amenity_network(self, G, communities):\n",
    "        \"\"\"Visualize the amenity network\"\"\"\n",
    "        fig = plt.figure(figsize=(15, 10))\n",
    "        ax = fig.add_subplot(111)\n",
    "        \n",
    "        # Create layout\n",
    "        pos = nx.spring_layout(G, k=1, iterations=50)\n",
    "        \n",
    "        # Color nodes by community\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(communities)))\n",
    "        node_colors = []\n",
    "        for node in G.nodes():\n",
    "            for i, community in enumerate(communities):\n",
    "                if node in community:\n",
    "                    node_colors.append(colors[i])\n",
    "                    break\n",
    "            else:\n",
    "                node_colors.append('gray')\n",
    "        \n",
    "        nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=50, alpha=0.7, ax=ax)\n",
    "        nx.draw_networkx_edges(G, pos, alpha=0.3, width=0.5, ax=ax)\n",
    "        \n",
    "        ax.set_title(\"Amenity Co-occurrence Network with Community Detection\", fontsize=16)\n",
    "        ax.axis('off')\n",
    "        fig.tight_layout()\n",
    "        save_figure(fig, \"fig_amenity_network.png\")\n",
    "        plt.close(fig)\n",
    "    \n",
    "    def stage2_clustering_analysis(self):\n",
    "        \"\"\"Stage 2: Multi-Stage Clustering of Listings\"\"\"\n",
    "        print(\"\\n=== Stage 2: Listing Clustering ===\")\n",
    "        \n",
    "        # Prepare features for clustering\n",
    "        feature_columns = [col for col in self.df_listings.columns \n",
    "                          if col.startswith('Profile_Category_') and col.endswith('_Count')]\n",
    "        \n",
    "        X = self.df_listings[feature_columns].fillna(0)\n",
    "        \n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Find optimal clustering\n",
    "        best_solution, results_df = self._find_optimal_clustering(X_scaled, feature_columns)\n",
    "        save_dataframe(results_df, \"clustering_model_comparison.csv\")\n",
    "        \n",
    "        # Bootstrap stability analysis\n",
    "        stability_results = self._bootstrap_stability_analysis(X_scaled, best_solution)\n",
    "        \n",
    "        # Assign final clusters\n",
    "        self.df_listings['Final_Cluster'] = best_solution['labels']\n",
    "        \n",
    "        # Profile clusters\n",
    "        cluster_profiles = self._profile_clusters(feature_columns)\n",
    "        cluster_profile_df = pd.DataFrame(cluster_profiles).T\n",
    "        save_dataframe(cluster_profile_df, \"cluster_profiles.csv\")\n",
    "        \n",
    "        # Visualize clustering results\n",
    "        self._visualize_clustering_results(X_scaled, best_solution['labels'], feature_columns)\n",
    "        \n",
    "        return cluster_profiles, stability_results\n",
    "    \n",
    "    def _find_optimal_clustering(self, X_scaled, feature_columns):\n",
    "        \"\"\"Find optimal clustering solution\"\"\"\n",
    "        print(\"Finding optimal clustering solution...\")\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for algorithm in ['kmeans', 'agglomerative']:\n",
    "            for k in range(2, 8):\n",
    "                if algorithm == 'kmeans':\n",
    "                    clusterer = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "                else:\n",
    "                    clusterer = AgglomerativeClustering(n_clusters=k)\n",
    "                \n",
    "                labels = clusterer.fit_predict(X_scaled)\n",
    "                \n",
    "                silhouette = silhouette_score(X_scaled, labels)\n",
    "                ch_score = calinski_harabasz_score(X_scaled, labels)\n",
    "                db_score = davies_bouldin_score(X_scaled, labels)\n",
    "                \n",
    "                results.append({\n",
    "                    'algorithm': algorithm,\n",
    "                    'k': k,\n",
    "                    'silhouette': silhouette,\n",
    "                    'ch_score': ch_score,\n",
    "                    'db_score': db_score,\n",
    "                    'labels': labels\n",
    "                })\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        # Normalize scores and calculate composite score\n",
    "        for col in ['silhouette', 'ch_score', 'db_score']:\n",
    "            if results_df[col].max() != results_df[col].min():\n",
    "                if col == 'db_score':\n",
    "                    results_df[col + '_norm'] = 1 - (results_df[col] - results_df[col].min()) / (results_df[col].max() - results_df[col].min())\n",
    "                else:\n",
    "                    results_df[col + '_norm'] = (results_df[col] - results_df[col].min()) / (results_df[col].max() - results_df[col].min())\n",
    "            else:\n",
    "                results_df[col + '_norm'] = 1.0\n",
    "        \n",
    "        results_df['composite_score'] = (results_df['silhouette_norm'] + results_df['ch_score_norm'] + results_df['db_score_norm']) / 3\n",
    "        \n",
    "        # Find best solution\n",
    "        best_idx = results_df['composite_score'].idxmax()\n",
    "        best_solution = results_df.loc[best_idx].to_dict()\n",
    "        \n",
    "        print(f\"Best solution: {best_solution['algorithm']} with k={int(best_solution['k'])}\")\n",
    "        print(f\"Composite score: {best_solution['composite_score']:.3f}\")\n",
    "        \n",
    "        append_log(f\"Best clustering solution: {best_solution}\")\n",
    "        return best_solution, results_df\n",
    "    \n",
    "    def _bootstrap_stability_analysis(self, X_scaled, best_solution, n_bootstrap=100):\n",
    "        \"\"\"Bootstrap stability analysis\"\"\"\n",
    "        print(\"Performing bootstrap stability analysis...\")\n",
    "        \n",
    "        k = int(best_solution['k'])\n",
    "        original_labels = best_solution['labels']\n",
    "        ari_scores = []\n",
    "        \n",
    "        for i in range(n_bootstrap):\n",
    "            bootstrap_indices = np.random.choice(len(X_scaled), len(X_scaled), replace=True)\n",
    "            X_bootstrap = X_scaled[bootstrap_indices]\n",
    "            \n",
    "            if best_solution['algorithm'] == 'kmeans':\n",
    "                clusterer = KMeans(n_clusters=k, random_state=i, n_init=10)\n",
    "            else:\n",
    "                clusterer = AgglomerativeClustering(n_clusters=k)\n",
    "            \n",
    "            bootstrap_labels = clusterer.fit_predict(X_bootstrap)\n",
    "            original_bootstrap_labels = original_labels[bootstrap_indices]\n",
    "            \n",
    "            ari = adjusted_rand_score(original_bootstrap_labels, bootstrap_labels)\n",
    "            ari_scores.append(ari)\n",
    "        \n",
    "        mean_ari = np.mean(ari_scores)\n",
    "        std_ari = np.std(ari_scores)\n",
    "        \n",
    "        print(f\"Bootstrap stability - Mean ARI: {mean_ari:.3f} ± {std_ari:.3f}\")\n",
    "        append_log(f\"Bootstrap ARI mean={mean_ari:.3f}, std={std_ari:.3f}\")\n",
    "        \n",
    "        # Save ARI distribution\n",
    "        ari_df = pd.DataFrame({\"bootstrap_iter\": range(n_bootstrap), \"ARI\": ari_scores})\n",
    "        save_dataframe(ari_df, \"clustering_bootstrap_ari.csv\")\n",
    "        \n",
    "        return {'ari_scores': ari_scores, 'mean_ari': mean_ari, 'std_ari': std_ari}\n",
    "    \n",
    "    def _profile_clusters(self, feature_columns):\n",
    "        \"\"\"Profile the clusters\"\"\"\n",
    "        print(\"Profiling clusters...\")\n",
    "        \n",
    "        cluster_profiles = {}\n",
    "        \n",
    "        for cluster_id in sorted(self.df_listings['Final_Cluster'].unique()):\n",
    "            cluster_data = self.df_listings[self.df_listings['Final_Cluster'] == cluster_id]\n",
    "            \n",
    "            profile = {\n",
    "                'size': len(cluster_data),\n",
    "                'share': len(cluster_data) / len(self.df_listings),\n",
    "                'avg_amenities': cluster_data[feature_columns].sum(axis=1).mean(),\n",
    "                'feature_means': cluster_data[feature_columns].mean().to_dict()\n",
    "            }\n",
    "            \n",
    "            cluster_profiles[cluster_id] = profile\n",
    "            \n",
    "            print(f\"\\nCluster {cluster_id}:\")\n",
    "            print(f\"  Size: {profile['size']} ({profile['share']:.1%})\")\n",
    "            print(f\"  Average amenities: {profile['avg_amenities']:.1f}\")\n",
    "        \n",
    "        return cluster_profiles\n",
    "    \n",
    "    def _visualize_clustering_results(self, X_scaled, labels, feature_columns):\n",
    "        \"\"\"Visualize clustering results\"\"\"\n",
    "        pca = PCA(n_components=2)\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # PCA scatter plot\n",
    "        scatter = axes[0, 0].scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', alpha=0.6)\n",
    "        axes[0, 0].set_title('PCA Projection of Listings by Cluster')\n",
    "        axes[0, 0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "        axes[0, 0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "        plt.colorbar(scatter, ax=axes[0, 0])\n",
    "        \n",
    "        # Cluster size distribution\n",
    "        cluster_counts = pd.Series(labels).value_counts().sort_index()\n",
    "        axes[0, 1].bar(range(len(cluster_counts)), cluster_counts.values, color='skyblue')\n",
    "        axes[0, 1].set_title('Cluster Size Distribution')\n",
    "        axes[0, 1].set_xlabel('Cluster')\n",
    "        axes[0, 1].set_ylabel('Number of Listings')\n",
    "        \n",
    "        # Feature heatmap\n",
    "        cluster_means = pd.DataFrame()\n",
    "        for cluster_id in sorted(np.unique(labels)):\n",
    "            cluster_mask = labels == cluster_id\n",
    "            means = pd.Series(X_scaled[cluster_mask].mean(axis=0), \n",
    "                              index=[col.replace('Profile_Category_', '').replace('_Count', '') \n",
    "                                     for col in feature_columns])\n",
    "            cluster_means[f'Cluster_{cluster_id}'] = means\n",
    "    \n",
    "        sns.heatmap(cluster_means.T, annot=True, cmap='RdBu_r', center=0, \n",
    "                    cbar_kws={'label': 'Standardized Feature Value'}, ax=axes[1, 0])\n",
    "        axes[1, 0].set_title('Standardized Feature Values by Cluster')\n",
    "        axes[1, 0].set_xlabel('')\n",
    "        axes[1, 0].set_ylabel('Clusters')\n",
    "        \n",
    "        axes[1, 1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        save_figure(fig, \"fig_clustering_results.png\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    def generate_comprehensive_report(self):\n",
    "        \"\"\"Generate a comprehensive analysis report and save text version\"\"\"\n",
    "        print(\"\\n=== COMPREHENSIVE ICC ANALYSIS REPORT ===\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        lines = []\n",
    "        lines.append(\"=== COMPREHENSIVE ICC ANALYSIS REPORT ===\")\n",
    "        lines.append(\"=\"*50)\n",
    "        \n",
    "        lines.append(f\"Dataset Overview:\")\n",
    "        lines.append(f\"- Total listings analyzed: {len(self.df_listings)}\")\n",
    "        lines.append(f\"- Price range: ${self.df_listings['price'].min():.0f} - ${self.df_listings['price'].max():.0f}\")\n",
    "        lines.append(f\"- Average price: ${self.df_listings['price'].mean():.0f}\")\n",
    "        \n",
    "        if 'Final_Cluster' in self.df_listings.columns:\n",
    "            cluster_summary = self.df_listings['Final_Cluster'].value_counts().sort_index()\n",
    "            lines.append(\"\\nListing Clusters:\")\n",
    "            for cluster, count in cluster_summary.items():\n",
    "                pct = count / len(self.df_listings) * 100\n",
    "                lines.append(f\"- Cluster {cluster}: {count} listings ({pct:.1f}%)\")\n",
    "        \n",
    "        amenity_cols = [col for col in self.df_listings.columns \n",
    "                       if col.startswith('Profile_Category_') and col.endswith('_Count')]\n",
    "        lines.append(\"\\nAmenity Categories Summary:\")\n",
    "        for col in amenity_cols:\n",
    "            avg_count = self.df_listings[col].mean()\n",
    "            category_name = col.replace('Profile_Category_', '').replace('_Count', '')\n",
    "            lines.append(f\"- {category_name}: {avg_count:.2f} average amenities\")\n",
    "        \n",
    "        lines.append(\"\\nAnalysis completed successfully!\")\n",
    "        lines.append(\"=\"*50)\n",
    "        \n",
    "        report_text = \"\\n\".join(lines)\n",
    "        print(report_text)\n",
    "        \n",
    "        # Save report text\n",
    "        report_path = os.path.join(output_dir, \"icc_comprehensive_report.txt\")\n",
    "        with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(report_text)\n",
    "        print(f\"[Saved report] {report_path}\")\n",
    "\n",
    "\n",
    "# Main execution function\n",
    "def run_stage1_stage2_analysis():\n",
    "    \"\"\"Run only Stage 1 and Stage 2 analysis\"\"\"\n",
    "    \n",
    "    pipeline = ICCAnalysisPipeline()\n",
    "    \n",
    "    try:\n",
    "        pipeline.load_data(files)\n",
    "        pipeline.preprocess_data()\n",
    "        \n",
    "        amenity_network, communities = pipeline.stage1_amenity_analysis()\n",
    "        cluster_profiles, stability_results = pipeline.stage2_clustering_analysis()\n",
    "        pipeline.generate_comprehensive_report()\n",
    "        \n",
    "        print(\"\\nStage 1 & 2 Analysis completed successfully!\")\n",
    "        \n",
    "        return pipeline\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Analysis failed with error: {e}\")\n",
    "        append_log(f\"Pipeline failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = run_stage1_stage2_analysis()\n",
    "    \n",
    "    if results:\n",
    "        print(\"\\nAnalysis completed. Results stored in 'results' variable.\")\n",
    "        print(\"Access the processed data via: results.df_listings\")\n",
    "    else:\n",
    "        print(\"Analysis failed. Please check the error messages above and icc_log.txt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d3a1569-7227-4601-b50a-94b4867b756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###'STEP 3: ESDA/MGWR' & 'STEP 4: ML VALIDATION (XGBoost & SHAP)'###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c67c9028-8487-47f6-b6c8-7e86b260d012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Hong Kong Airbnb Spatial Analysis Pipeline Initialized ===\n",
      "Output Directory: C:\\Users\\USER\\Downloads\\ICC in Digital Platforms\\STAGE 3&4_outputs\n",
      "\n",
      "[Phase 1] Loading and Preprocessing Data...\n",
      "  - Loaded Raw Data: (2928, 136)\n",
      "  - Loaded Boundaries: 18 districts\n",
      "  - Final Cleaned Spatial Data: (2927, 137)\n",
      "  - Amenity Categories Found: 10\n",
      "\n",
      "[Phase 2] Step 3-1: Exploratory Spatial Data Analysis (ESDA)...\n",
      "  - Running KDE Analysis...\n",
      "  - Running Hotspot Analysis (Getis-Ord Gi*)...\n",
      "  - Generating Interactive Maps...\n",
      "\n",
      "[Phase 3] Step 3-2: Spatial Modeling (MGWR)...\n",
      "\n",
      "  --- Processing Price_Model (log_price) ---\n",
      "    > Running OLS and checking residuals...\n",
      "      OLS Residual Moran's I: 0.2187 (p-value: 0.0010)\n",
      "    > Running MGWR (This may take time)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5b265662664d43a67b2334d7bc1330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Backfitting:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Selected Bandwidths: [  49.  241.  280. 1463. 1706.  157. 2925.  259. 1775. 2676.  241.  785.\n",
      " 2926. 2504.]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9ba11b997049e691efbaae2a851d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      MGWR R2: 0.7022\n",
      "    > Visualizing Coefficients for Price_Model...\n",
      "\n",
      "  --- Processing Rating_Model (review_scores_rating) ---\n",
      "    > Running OLS and checking residuals...\n",
      "      OLS Residual Moran's I: 0.0077 (p-value: 0.1620)\n",
      "    > Running MGWR (This may take time)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a72916d9814558b4670382304a93c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Backfitting:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Selected Bandwidths: [ 199. 2083.   85.   58.   48.   53.   58.   44.]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "017be8502ea74e308fa92b526c7252a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      MGWR R2: 0.9359\n",
      "    > Visualizing Coefficients for Rating_Model...\n",
      "\n",
      "[Phase 4] Step 4: ML Validation (XGBoost & SHAP)...\n",
      "  - Training XGBoost for log_price...\n",
      "    > Test R2 Score: 0.5579\n",
      "  - Calculating SHAP Values...\n",
      "\n",
      "=== Analysis Pipeline Completed Successfully ===\n",
      "Results saved in: C:\\Users\\USER\\Downloads\\ICC in Digital Platforms\\STAGE 3&4_outputs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from math import ceil\n",
    "import warnings\n",
    "\n",
    "# Spatial Analysis Libraries\n",
    "try:\n",
    "    from esda.getisord import G_Local\n",
    "    from esda.moran import Moran\n",
    "    from libpysal.weights import KNN\n",
    "    from mgwr.gwr import GWR, MGWR\n",
    "    from mgwr.sel_bw import Sel_BW\n",
    "except ImportError:\n",
    "    print(\"CRITICAL: Please install spatial libraries: pip install esda libpysal mgwr\")\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'  # Standard font for English\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURATION & PATHS\n",
    "# ==============================================================================\n",
    "CONFIG = {\n",
    "    \"base_path\": r\"C:\\Users\\USER\\Downloads\\ICC in Digital Platforms\",\n",
    "    \"input_file\": \"listings_Add_Amenity.xlsx\",\n",
    "    \"geojson_file\": \"neighbourhoods.geojson\",\n",
    "    \"output_dir_name\": \"STAGE 3&4_outputs\",\n",
    "    # Hong Kong Bounding Box\n",
    "    \"bounds\": {'min_lon': 113.83, 'max_lon': 114.41, 'min_lat': 22.15, 'max_lat': 22.57}\n",
    "}\n",
    "\n",
    "class HongKongSpatialAnalysis:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.base_path = config['base_path']\n",
    "        self.output_dir = os.path.join(self.base_path, config['output_dir_name'])\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        self.df = None\n",
    "        self.gdf = None\n",
    "        self.gdf_boundaries = None\n",
    "        self.category_vars = []\n",
    "        \n",
    "        print(\"=== Hong Kong Airbnb Spatial Analysis Pipeline Initialized ===\")\n",
    "        print(f\"Output Directory: {self.output_dir}\")\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 1. DATA LOADING & PREPROCESSING\n",
    "    # ==========================================================================\n",
    "    def load_and_preprocess(self):\n",
    "        print(\"\\n[Phase 1] Loading and Preprocessing Data...\")\n",
    "        \n",
    "        file_path = os.path.join(self.base_path, self.config['input_file'])\n",
    "        geojson_path = os.path.join(self.base_path, self.config['geojson_file'])\n",
    "        \n",
    "        # 1. Load Excel\n",
    "        try:\n",
    "            self.df = pd.read_excel(file_path)\n",
    "            print(f\"  - Loaded Raw Data: {self.df.shape}\")\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Input file not found: {file_path}\")\n",
    "\n",
    "        # 2. Load GeoJSON\n",
    "        try:\n",
    "            self.gdf_boundaries = gpd.read_file(geojson_path)\n",
    "            print(f\"  - Loaded Boundaries: {len(self.gdf_boundaries)} districts\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - Warning: Could not load GeoJSON ({e}). Maps will lack boundaries.\")\n",
    "\n",
    "        # 3. Cleaning\n",
    "        # Drop rows without coordinates or target variables\n",
    "        subset_cols = ['latitude', 'longitude', 'log_price', 'review_scores_rating']\n",
    "        self.df.dropna(subset=subset_cols, inplace=True)\n",
    "        \n",
    "        # Identify Amenity Category Variables\n",
    "        self.category_vars = [col for col in self.df.columns \n",
    "                              if col.startswith('Profile_Category_') and col.endswith('_Count') \n",
    "                              and col != 'Profile_Category_Other_Count']\n",
    "        \n",
    "        # Impute missing numeric values with median (for modeling stability)\n",
    "        numeric_cols = self.df.select_dtypes(include=np.number).columns\n",
    "        for col in numeric_cols:\n",
    "            if self.df[col].isnull().any():\n",
    "                median_val = self.df[col].median()\n",
    "                self.df[col].fillna(median_val, inplace=True)\n",
    "\n",
    "        # 4. Create GeoDataFrame\n",
    "        self.gdf = gpd.GeoDataFrame(\n",
    "            self.df, \n",
    "            geometry=gpd.points_from_xy(self.df.longitude, self.df.latitude),\n",
    "            crs='EPSG:4326'\n",
    "        )\n",
    "\n",
    "        # 5. Spatial Filtering (Hong Kong Bounds)\n",
    "        b = self.config['bounds']\n",
    "        self.gdf = self.gdf.cx[b['min_lon']:b['max_lon'], b['min_lat']:b['max_lat']].copy()\n",
    "        \n",
    "        print(f\"  - Final Cleaned Spatial Data: {self.gdf.shape}\")\n",
    "        print(f\"  - Amenity Categories Found: {len(self.category_vars)}\")\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 2. STEP 3-1: ESDA (KDE & Hotspot)\n",
    "    # ==========================================================================\n",
    "    def run_esda_analysis(self):\n",
    "        print(\"\\n[Phase 2] Step 3-1: Exploratory Spatial Data Analysis (ESDA)...\")\n",
    "        \n",
    "        # 타겟 변수 설정\n",
    "        target_vars = ['log_price', 'review_scores_rating'] + self.category_vars\n",
    "        \n",
    "        # 1) KDE 분석\n",
    "        self._perform_kde(target_vars)\n",
    "        \n",
    "        # 2) Hotspot Analysis (Getis-Ord Gi*)\n",
    "        hotspot_results = self._perform_hotspot(target_vars)\n",
    "        \n",
    "        # 3) 시각화\n",
    "        self._visualize_hotspots(hotspot_results)\n",
    "        self._create_interactive_maps(hotspot_results)\n",
    "\n",
    "    def _perform_kde(self, vars_to_analyze):\n",
    "        print(\"  - Running KDE Analysis...\")\n",
    "        b = self.config['bounds']\n",
    "        xx, yy = np.mgrid[b['min_lon']:b['max_lon']:100j, b['min_lat']:b['max_lat']:100j]\n",
    "        positions = np.vstack([xx.ravel(), yy.ravel()])\n",
    "        \n",
    "        kde_results = {}\n",
    "        \n",
    "        for var in vars_to_analyze:\n",
    "            if var not in self.gdf.columns: continue\n",
    "            \n",
    "            try:\n",
    "                values = self.gdf[var].values\n",
    "                coords = np.vstack([self.gdf.longitude, self.gdf.latitude])\n",
    "                # Weighted KDE\n",
    "                kernel = gaussian_kde(coords, weights=values)\n",
    "                density = np.reshape(kernel(positions).T, xx.shape)\n",
    "                kde_results[var] = density\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        # Visualize KDE\n",
    "        if kde_results:\n",
    "            n = len(kde_results)\n",
    "            cols = 3\n",
    "            rows = ceil(n / cols)\n",
    "            fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 4*rows))\n",
    "            axes = axes.flatten()\n",
    "            \n",
    "            for i, (var, density) in enumerate(kde_results.items()):\n",
    "                ax = axes[i]\n",
    "                title = var.replace(\"Profile_Category_\", \"\").replace(\"_Count\", \"\")\n",
    "                \n",
    "                # Plot density\n",
    "                im = ax.contourf(xx, yy, density, levels=15, cmap='hot', alpha=0.85)\n",
    "                \n",
    "                # Overlay boundaries\n",
    "                if self.gdf_boundaries is not None:\n",
    "                    self.gdf_boundaries.plot(ax=ax, edgecolor='white', facecolor='none', linewidth=0.5)\n",
    "                \n",
    "                ax.set_title(title, fontsize=10)\n",
    "                ax.axis('off')\n",
    "                \n",
    "            # Hide unused subplots\n",
    "            for j in range(i + 1, len(axes)):\n",
    "                axes[j].axis('off')\n",
    "                \n",
    "            plt.suptitle(\"Kernel Density Estimation (KDE) Heatmaps\", fontsize=16, y=1.02)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(self.output_dir, \"ESDA_KDE_Heatmaps.png\"), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "    def _perform_hotspot(self, vars_to_analyze):\n",
    "        print(\"  - Running Hotspot Analysis (Getis-Ord Gi*)...\")\n",
    "        results = {}\n",
    "        \n",
    "        from libpysal.weights import KNN\n",
    "        from esda import G_Local\n",
    "        \n",
    "        # Spatial Weights (KNN)\n",
    "        w = KNN.from_dataframe(self.gdf, k=8)\n",
    "        w.transform = 'r'\n",
    "        \n",
    "        for var in vars_to_analyze:\n",
    "            if var not in self.gdf.columns:\n",
    "                continue\n",
    "            \n",
    "            # float64 변환\n",
    "            y = self.gdf[var].astype(np.float64).values\n",
    "            \n",
    "            # n_jobs=1로 Numba 오류 회피\n",
    "            gi = G_Local(y, w, transform='B', permutations=999, n_jobs=1)\n",
    "            \n",
    "            results[var] = {\n",
    "                'z_score': gi.z_sim,\n",
    "                'p_value': gi.p_sim,\n",
    "                'hot': (gi.p_sim < 0.05) & (gi.z_sim > 1.96),\n",
    "                'cold': (gi.p_sim < 0.05) & (gi.z_sim < -1.96)\n",
    "            }\n",
    "        return results\n",
    "\n",
    "    def _visualize_hotspots(self, results):\n",
    "        if not results: return\n",
    "        \n",
    "        n = len(results)\n",
    "        cols = 3\n",
    "        rows = ceil(n / cols)\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 4*rows))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, (var, res) in enumerate(results.items()):\n",
    "            ax = axes[i]\n",
    "            title = var.replace(\"Profile_Category_\", \"\").replace(\"_Count\", \"\")\n",
    "            \n",
    "            # Background\n",
    "            if self.gdf_boundaries is not None:\n",
    "                self.gdf_boundaries.plot(ax=ax, color='#f0f0f0', edgecolor='white')\n",
    "            \n",
    "            # Non-significant points\n",
    "            ax.scatter(self.gdf.longitude, self.gdf.latitude, c='lightgray', s=1, alpha=0.3)\n",
    "            \n",
    "            # Hot spots (Red)\n",
    "            hot_mask = res['hot']\n",
    "            if hot_mask.any():\n",
    "                ax.scatter(self.gdf.loc[hot_mask, 'longitude'], self.gdf.loc[hot_mask, 'latitude'], \n",
    "                           c='red', s=5, label='Hot Spot')\n",
    "                \n",
    "            # Cold spots (Blue)\n",
    "            cold_mask = res['cold']\n",
    "            if cold_mask.any():\n",
    "                ax.scatter(self.gdf.loc[cold_mask, 'longitude'], self.gdf.loc[cold_mask, 'latitude'], \n",
    "                           c='blue', s=5, label='Cold Spot')\n",
    "            \n",
    "            ax.set_title(title, fontsize=10)\n",
    "            ax.axis('off')\n",
    "\n",
    "        for j in range(i + 1, len(axes)):\n",
    "            axes[j].axis('off')\n",
    "            \n",
    "        plt.suptitle(\"Hotspot Analysis (Getis-Ord Gi*)\", fontsize=16, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, \"ESDA_Hotspots.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    def _create_interactive_maps(self, results):\n",
    "        print(\"  - Generating Interactive Maps...\")\n",
    "        center = [22.3193, 114.1694]\n",
    "        \n",
    "        for var, res in results.items():\n",
    "            m = folium.Map(location=center, zoom_start=11, tiles='CartoDB positron')\n",
    "            \n",
    "            # Add GeoJSON boundary if available\n",
    "            if self.gdf_boundaries is not None:\n",
    "                folium.GeoJson(\n",
    "                    self.gdf_boundaries,\n",
    "                    style_function=lambda x: {'color': 'black', 'weight': 0.5, 'fillOpacity': 0}\n",
    "                ).add_to(m)\n",
    "            \n",
    "            # Add Hot/Cold spots\n",
    "            hot_group = folium.FeatureGroup(name='Hot Spots')\n",
    "            cold_group = folium.FeatureGroup(name='Cold Spots')\n",
    "            \n",
    "            # Filter data for mapping\n",
    "            hot_df = self.gdf[res['hot']]\n",
    "            cold_df = self.gdf[res['cold']]\n",
    "            \n",
    "            for _, row in hot_df.iterrows():\n",
    "                folium.CircleMarker(\n",
    "                    [row.latitude, row.longitude], radius=3, color='red', fill=True, fill_opacity=0.7,\n",
    "                    popup=f\"{var}: Hot Spot\"\n",
    "                ).add_to(hot_group)\n",
    "                \n",
    "            for _, row in cold_df.iterrows():\n",
    "                folium.CircleMarker(\n",
    "                    [row.latitude, row.longitude], radius=3, color='blue', fill=True, fill_opacity=0.7,\n",
    "                    popup=f\"{var}: Cold Spot\"\n",
    "                ).add_to(cold_group)\n",
    "                \n",
    "            hot_group.add_to(m)\n",
    "            cold_group.add_to(m)\n",
    "            folium.LayerControl().add_to(m)\n",
    "            \n",
    "            clean_name = var.replace(\"Profile_Category_\", \"\").replace(\"_Count\", \"\")\n",
    "            m.save(os.path.join(self.output_dir, f\"Interactive_Map_{clean_name}.html\"))\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 3. STEP 3-2: SPATIAL MODELING (MGWR)\n",
    "    # ==========================================================================\n",
    "    def run_spatial_modeling(self):\n",
    "        print(\"\\n[Phase 3] Step 3-2: Spatial Modeling (MGWR)...\")\n",
    "        \n",
    "        # Define models\n",
    "        models = {\n",
    "            'Price_Model': {\n",
    "                'dep': 'log_price',\n",
    "                'indep': [\n",
    "                    'log_accommodates', 'log_bedrooms', 'host_is_superhost',\n",
    "                    'Profile_Category_Business_Work_Count',\n",
    "                    'Profile_Category_Transportation_Count',\n",
    "                    'Profile_Category_Safety_Security_Count',\n",
    "                    'Profile_Category_Accessibility_Count',\n",
    "                    'Profile_Category_Essential_Tech_Count',\n",
    "                    'Profile_Category_Kitchen_Dining_Count',\n",
    "                    'Profile_Category_Bathroom_Personal_Count',\n",
    "                    'Profile_Category_Outdoor_Recreation_Count',\n",
    "                    'Profile_Category_Bedding_Comfort_Count',\n",
    "                    'Profile_Category_Climate_Control_Count'\n",
    "                ]\n",
    "            },\n",
    "            'Rating_Model': {\n",
    "                'dep': 'review_scores_rating',\n",
    "                'indep': [\n",
    "                    'host_is_superhost', 'host_response_rate',\n",
    "                    'review_scores_cleanliness', 'review_scores_communication',\n",
    "                    'review_scores_checkin', 'review_scores_location', 'review_scores_value'\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for model_name, config in models.items():\n",
    "            self._execute_mgwr_pipeline(model_name, config['dep'], config['indep'])\n",
    "\n",
    "    def _execute_mgwr_pipeline(self, model_name, dep_var, indep_vars):\n",
    "        print(f\"\\n  --- Processing {model_name} ({dep_var}) ---\")\n",
    "        \n",
    "        # Prepare Data\n",
    "        y = self.gdf[dep_var].values.reshape(-1, 1)\n",
    "        X = self.gdf[indep_vars].values\n",
    "        coords = list(zip(self.gdf.longitude, self.gdf.latitude))\n",
    "        \n",
    "        # Standardize\n",
    "        scaler_X = StandardScaler()\n",
    "        scaler_y = StandardScaler()\n",
    "        X_scaled = scaler_X.fit_transform(X)\n",
    "        y_scaled = scaler_y.fit_transform(y)\n",
    "        \n",
    "        # 3.1 OLS & Moran's I\n",
    "        print(\"    > Running OLS and checking residuals...\")\n",
    "        # Simple OLS for residual check\n",
    "        X_ols = np.hstack([np.ones(y_scaled.shape), X_scaled])\n",
    "        betas = np.linalg.inv(X_ols.T @ X_ols) @ X_ols.T @ y_scaled\n",
    "        residuals = y_scaled - X_ols @ betas\n",
    "        \n",
    "        w = KNN.from_dataframe(self.gdf, k=8)\n",
    "        w.transform = 'r'\n",
    "        moran = Moran(residuals, w)\n",
    "        print(f\"      OLS Residual Moran's I: {moran.I:.4f} (p-value: {moran.p_sim:.4f})\")\n",
    "        \n",
    "        # 3.2 MGWR\n",
    "        print(\"    > Running MGWR (This may take time)...\")\n",
    "        selector = Sel_BW(coords, y_scaled, X_scaled, multi=True, spherical=True)\n",
    "        bw = selector.search()\n",
    "        print(f\"      Selected Bandwidths: {bw}\")\n",
    "        \n",
    "        mgwr_model = MGWR(coords, y_scaled, X_scaled, selector=selector, spherical=True)\n",
    "        results = mgwr_model.fit()\n",
    "        print(f\"      MGWR R2: {results.R2:.4f}\")\n",
    "        \n",
    "        # 3.3 Visualize Coefficients\n",
    "        self._visualize_mgwr_coeffs(model_name, results, indep_vars)\n",
    "\n",
    "    def _visualize_mgwr_coeffs(self, model_name, results, var_names):\n",
    "        print(f\"    > Visualizing Coefficients for {model_name}...\")\n",
    "        \n",
    "        n = len(var_names)\n",
    "        cols = 3\n",
    "        rows = ceil(n / cols)\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 4*rows))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, var in enumerate(var_names):\n",
    "            ax = axes[i]\n",
    "            # Extract coefficients (column i in params)\n",
    "            coeffs = results.params[:, i]\n",
    "            \n",
    "            # Plot\n",
    "            if self.gdf_boundaries is not None:\n",
    "                self.gdf_boundaries.plot(ax=ax, color='whitesmoke', edgecolor='black', linewidth=0.3)\n",
    "            \n",
    "            sc = ax.scatter(self.gdf.longitude, self.gdf.latitude, c=coeffs, cmap='RdBu_r', s=5, alpha=0.8)\n",
    "            plt.colorbar(sc, ax=ax, fraction=0.046, pad=0.04)\n",
    "            \n",
    "            clean_title = var.replace(\"Profile_Category_\", \"\").replace(\"_Count\", \"\")\n",
    "            ax.set_title(f\"Coeff: {clean_title}\", fontsize=9)\n",
    "            ax.axis('off')\n",
    "            \n",
    "        for j in range(i + 1, len(axes)):\n",
    "            axes[j].axis('off')\n",
    "            \n",
    "        plt.suptitle(f\"MGWR Local Coefficients: {model_name}\", fontsize=16, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, f\"MGWR_Coeffs_{model_name}.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 4. STEP 4: ML VALIDATION (XGBoost & SHAP)\n",
    "    # ==========================================================================\n",
    "    def run_ml_validation(self):\n",
    "        print(\"\\n[Phase 4] Step 4: ML Validation (XGBoost & SHAP)...\")\n",
    "        \n",
    "        # We will focus on the Price Model for validation example\n",
    "        target = 'log_price'\n",
    "        features = [\n",
    "            'log_accommodates', 'log_bedrooms', 'host_is_superhost',\n",
    "            'Profile_Category_Business_Work_Count',\n",
    "            'Profile_Category_Transportation_Count',\n",
    "            'Profile_Category_Safety_Security_Count',\n",
    "            'Profile_Category_Accessibility_Count',\n",
    "            'Profile_Category_Essential_Tech_Count',\n",
    "            'Profile_Category_Kitchen_Dining_Count',\n",
    "            'Profile_Category_Bathroom_Personal_Count',\n",
    "            'Profile_Category_Outdoor_Recreation_Count',\n",
    "            'Profile_Category_Bedding_Comfort_Count',\n",
    "            'Profile_Category_Climate_Control_Count'\n",
    "        ]\n",
    "        \n",
    "        print(f\"  - Training XGBoost for {target}...\")\n",
    "        \n",
    "        X = self.gdf[features]\n",
    "        y = self.gdf[target]\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # XGBoost Regressor (Updated parameters for newer versions)\n",
    "        model = xgb.XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            n_estimators=1000,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=6,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            early_stopping_rounds=50,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "        print(f\"    > Test R2 Score: {model.score(X_test, y_test):.4f}\")\n",
    "        \n",
    "        # SHAP Analysis\n",
    "        print(\"  - Calculating SHAP Values...\")\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X)\n",
    "        \n",
    "        # 1. Summary Plot (Beeswarm)\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X, show=False)\n",
    "        plt.title(f\"SHAP Summary: {target}\", fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, f\"SHAP_Summary_{target}.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # 2. Importance Bar Plot\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X, plot_type=\"bar\", show=False)\n",
    "        plt.title(f\"SHAP Feature Importance: {target}\", fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, f\"SHAP_Importance_{target}.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    # ==========================================================================\n",
    "    # MAIN EXECUTION\n",
    "    # ==========================================================================\n",
    "    def run_pipeline(self):\n",
    "        try:\n",
    "            self.load_and_preprocess()\n",
    "            self.run_esda_analysis()\n",
    "            self.run_spatial_modeling()\n",
    "            self.run_ml_validation()\n",
    "            print(\"\\n=== Analysis Pipeline Completed Successfully ===\")\n",
    "            print(f\"Results saved in: {self.output_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nCRITICAL ERROR: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize and run\n",
    "    analysis = HongKongSpatialAnalysis(CONFIG)\n",
    "    analysis.run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a5cc314-b283-41fe-8297-498b1ecc8f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
